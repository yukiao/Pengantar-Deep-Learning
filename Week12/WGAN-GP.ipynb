{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvYDZmH_xHPG"
      },
      "source": [
        "# Wasserstein GAN with Gradient Penalty (WGAN-GP)\n",
        "Based on paper [Improved Training of Wasserstein GANs](https://arxiv.org/abs/1704.00028)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HYN0DynDQKP"
      },
      "source": [
        "## Outline\n",
        "- Introduction\n",
        "- Prerequest\n",
        "- Datasets\n",
        "- Build Models\n",
        "    - Generator Models\n",
        "    - Discriminator Models\n",
        "- Models Settings\n",
        "- Training\n",
        "- Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRNaFouhDQKQ"
      },
      "source": [
        "### Introduction\n",
        "#### Abstract :\n",
        "Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.<br>\n",
        "\n",
        "![WGAN-GP](https://github.com/DSC-UI-SRIN/Introduction-to-GAN/raw/master/2%20-%20%20Wasserstein%20GANs/images/wgan-gp.png)<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3BnIRg8DQKQ"
      },
      "source": [
        "### Prerequest "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dgm42fIxDQKR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0CJryj5DQKS"
      },
      "outputs": [],
      "source": [
        "# import All prerequisites\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "ROOT = \"/content/drive/My Drive/Colab Notebooks/DSC_UI_GAN/Batch1/W2/WGAN-GP\"\n",
        "sample_dir = os.path.join(ROOT, 'sample')\n",
        "# Make dir if no exist\n",
        "if not os.path.exists(sample_dir):\n",
        "    os.makedirs(sample_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu6vT-jDxHPT"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SgpErv5DQKU"
      },
      "outputs": [],
      "source": [
        "batch_size = 100\n",
        "\n",
        "# MNIST Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "examples = enumerate(train_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLsBfy8eDQKU"
      },
      "outputs": [],
      "source": [
        "## Print example\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLypGXB6xHPb"
      },
      "source": [
        "## Build Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARMAgND0DQKV"
      },
      "source": [
        "#### Generator Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KY2T7ptMDQKV"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, g_input_dim, g_output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(g_input_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, g_output_dim), \n",
        "            nn.Tanh())\n",
        "\n",
        "    def forward(self, z):\n",
        "        image = self.model(z)\n",
        "        image = image.view(image.size(0), *example_data.shape[1:])\n",
        "        return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8a2b5G8DQKW"
      },
      "source": [
        "#### Discriminator Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CIkmU-sDQKW"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, d_input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(d_input_dim, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, image):\n",
        "        img_flat = image.view(image.size(0), -1)\n",
        "        validity = self.model(img_flat)\n",
        "        return validity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2mPLF3fDQKX"
      },
      "source": [
        "# Build network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hzkH76lDQKX"
      },
      "outputs": [],
      "source": [
        "# build network\n",
        "z_dim = 100\n",
        "mnist_dim = train_dataset.train_data.size(1) * train_dataset.train_data.size(2)\n",
        "\n",
        "G = Generator(g_input_dim = z_dim, g_output_dim = mnist_dim).to(device)\n",
        "D = Discriminator(mnist_dim).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_CAJMZjDQKX"
      },
      "outputs": [],
      "source": [
        "print(G, D)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw6SuIw-xHPm"
      },
      "source": [
        "# Train Process\n",
        "\n",
        "![WGAN Algorithm](https://github.com/DSC-UI-SRIN/Introduction-to-GAN/raw/master/2%20-%20%20Wasserstein%20GANs/images/wgan-gp-algorithm.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeXEDzmoDQKY"
      },
      "source": [
        "#### Gradient Penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGeiGvPtDQKZ"
      },
      "outputs": [],
      "source": [
        "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
        "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
        "    # Random weight term for interpolation between real and fake samples\n",
        "    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
        "    # Get random interpolation between real and fake samples\n",
        "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
        "    d_interpolates = D(interpolates)\n",
        "    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
        "    # Get gradient w.r.t. interpolates\n",
        "    gradients = autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=fake,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True,\n",
        "    )[0]\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    return gradient_penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyxpitC6DQKa"
      },
      "outputs": [],
      "source": [
        "# Loss weight for gradient penalty\n",
        "lambda_gp = 10\n",
        "\n",
        "# optimizer\n",
        "lr = 0.0002\n",
        "n_critic =  5\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "\n",
        "optimizer_G = torch.optim.Adam(G.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(b1, b2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niq21RdEDQKa"
      },
      "outputs": [],
      "source": [
        "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
        "epochs = 200\n",
        "list_loss_D = []\n",
        "list_loss_G = []\n",
        "for epoch in range(epochs):\n",
        "    for i, (imgs, _) in enumerate(train_loader):\n",
        "        # Configure input\n",
        "        real_imgs = Variable(imgs.type(Tensor))\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], z_dim))))\n",
        "\n",
        "        # Generate a batch of images\n",
        "        fake_imgs = G(z).detach()\n",
        "\n",
        "        # Gradient penalty\n",
        "        gradient_penalty = compute_gradient_penalty(D, real_imgs.data, fake_imgs.data)\n",
        "\n",
        "        # Adversarial loss\n",
        "        d_loss = -torch.mean(D(real_imgs)) + torch.mean(D(fake_imgs)) + lambda_gp * gradient_penalty\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train the generator every n_critic iterations\n",
        "        if i % n_critic == 0:\n",
        "\n",
        "            # -----------------\n",
        "            #  Train Generator\n",
        "            # -----------------\n",
        "\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            # Generate a batch of images\n",
        "            gen_imgs = G(z)\n",
        "            # Adversarial loss\n",
        "            g_loss = -torch.mean(D(gen_imgs))\n",
        "\n",
        "            g_loss.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            list_loss_D.append(d_loss.item())\n",
        "            list_loss_G.append(g_loss.item())\n",
        "        if i % 300 == 0:\n",
        "            print(\n",
        "              \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "              % (epoch, epochs, i, len(train_loader), d_loss.item(), g_loss.item()))\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        save_image(gen_imgs.view(gen_imgs.size(0), 1, 28, 28), os.path.join(sample_dir, \"%d.png\" % epoch), nrow=5, normalize=True)\n",
        "\n",
        "torch.save(G, os.path.join(ROOT, 'G.pt'))\n",
        "torch.save(D, os.path.join(ROOT, 'D.pt'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "WGAN-GP.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}