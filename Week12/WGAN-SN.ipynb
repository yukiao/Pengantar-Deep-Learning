{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvYDZmH_xHPG"
      },
      "source": [
        "# Wasserstein GAN with Spectral Normalization (WGAN-SN)\n",
        "Based on paper [Spectral Normalization for Generative Adversarial Networks](https://openreview.net/forum?id=B1QRgziT-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2OsrzMbEMXn"
      },
      "source": [
        "## Outline\n",
        "- Introduction\n",
        "- Prerequest\n",
        "- Datasets\n",
        "- Build Models\n",
        "    - Generator Models\n",
        "    - Discriminator Models\n",
        "- Models Settings\n",
        "- Training\n",
        "- Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUgO7OC5EMXo"
      },
      "source": [
        "### Introduction\n",
        "#### Abstract :\n",
        "One of the challenges in the study of generative adversarial networks is the insta-bility of its training. In this paper, we propose a novel weight normalization tech-nique called spectral normalization to stabilize the training of the discriminator.Our new normalization technique is computationally light and easy to incorporateinto existing implementations. We tested the efficacy of spectral normalization onCIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmedthat spectrally normalized GANs (SN-GANs) is capable of generating images ofbetter or equal quality relative to the previous training stabilization techniques.The code with Chainer (Tokui et al., 2015), generated images and pretrained mod-els  are  available  at https://github.com/pfnet-research/sngan_projection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmYRiXQwEMXp"
      },
      "source": [
        "### Prerequest "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndzJks0zEMXp"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfPMa7qJEMXq"
      },
      "outputs": [],
      "source": [
        "# import All prerequisites\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "ROOT = \"/content/drive/My Drive/Colab Notebooks/DSC_UI_GAN/Batch1/W2/WGAN-SN\"\n",
        "sample_dir = os.path.join(ROOT, 'sample')\n",
        "# Make dir if no exist\n",
        "if not os.path.exists(sample_dir):\n",
        "    os.makedirs(sample_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu6vT-jDxHPT"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBpHLlwFEMXr"
      },
      "outputs": [],
      "source": [
        "batch_size = 100\n",
        "\n",
        "# MNIST Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "examples = enumerate(train_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzVm0YoLEMXs"
      },
      "outputs": [],
      "source": [
        "## Print example\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEF1TEeAEMXt"
      },
      "source": [
        "## Make Spectral Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzieeMysEMXt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from torch.nn import Parameter\n",
        "\n",
        "def l2normalize(v, eps=1e-12):\n",
        "    return v / (v.norm() + eps)\n",
        "\n",
        "\n",
        "class SpectralNorm(nn.Module):\n",
        "    def __init__(self, module, name='weight', power_iterations=1):\n",
        "        super(SpectralNorm, self).__init__()\n",
        "        self.module = module\n",
        "        self.name = name\n",
        "        self.power_iterations = power_iterations\n",
        "        if not self._made_params():\n",
        "            self._make_params()\n",
        "\n",
        "    def _update_u_v(self):\n",
        "        u = getattr(self.module, self.name + \"_u\")\n",
        "        v = getattr(self.module, self.name + \"_v\")\n",
        "        w = getattr(self.module, self.name + \"_bar\")\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        for _ in range(self.power_iterations):\n",
        "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
        "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
        "\n",
        "        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n",
        "        sigma = u.dot(w.view(height, -1).mv(v))\n",
        "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
        "\n",
        "    def _made_params(self):\n",
        "        try:\n",
        "            u = getattr(self.module, self.name + \"_u\")\n",
        "            v = getattr(self.module, self.name + \"_v\")\n",
        "            w = getattr(self.module, self.name + \"_bar\")\n",
        "            return True\n",
        "        except AttributeError:\n",
        "            return False\n",
        "\n",
        "\n",
        "    def _make_params(self):\n",
        "        w = getattr(self.module, self.name)\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        width = w.view(height, -1).data.shape[1]\n",
        "\n",
        "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
        "        u.data = l2normalize(u.data)\n",
        "        v.data = l2normalize(v.data)\n",
        "        w_bar = Parameter(w.data)\n",
        "\n",
        "        del self.module._parameters[self.name]\n",
        "\n",
        "        self.module.register_parameter(self.name + \"_u\", u)\n",
        "        self.module.register_parameter(self.name + \"_v\", v)\n",
        "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
        "\n",
        "\n",
        "    def forward(self, *args):\n",
        "        self._update_u_v()\n",
        "        return self.module.forward(*args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLypGXB6xHPb"
      },
      "source": [
        "## Build Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhP5GojXEMXw"
      },
      "source": [
        "#### Generator Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wd3BJ_D_EMXw"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, g_input_dim, g_output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(g_input_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, g_output_dim), \n",
        "            nn.Tanh())\n",
        "\n",
        "    def forward(self, z):\n",
        "        image = self.model(z)\n",
        "        image = image.view(image.size(0), -1)\n",
        "        return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZPa0MRZEMXx"
      },
      "source": [
        "#### Discriminator Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRXpSsn-EMXx"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, d_input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            SpectralNorm(nn.Linear(d_input_dim, 512)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            SpectralNorm(nn.Linear(512, 256)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            SpectralNorm(nn.Linear(256, 1)),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, image):\n",
        "        img_flat = image.view(image.size(0), -1)\n",
        "        validity = self.model(img_flat)\n",
        "        return validity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3bB98PWEMXy"
      },
      "source": [
        "# Build network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fc0ZaeOpEMXy"
      },
      "outputs": [],
      "source": [
        "# build network\n",
        "z_dim = 100\n",
        "mnist_dim = train_dataset.train_data.size(1) * train_dataset.train_data.size(2)\n",
        "\n",
        "G = Generator(g_input_dim = z_dim, g_output_dim = mnist_dim).to(device)\n",
        "D = Discriminator(mnist_dim).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18nZYBc6EMXz"
      },
      "outputs": [],
      "source": [
        "print(G, D)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw6SuIw-xHPm"
      },
      "source": [
        "# Train Process\n",
        "![WGAN Algorithm](https://github.com/DSC-UI-SRIN/Introduction-to-GAN/raw/master/2%20-%20%20Wasserstein%20GANs/images/wgan_algorithm.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnINqsyZEMXz"
      },
      "outputs": [],
      "source": [
        "# optimizer\n",
        "lr = 0.0002\n",
        "n_critic =  5\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "\n",
        "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr, betas=(b1, b2))\n",
        "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr, betas=(b1, b2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P6izjZPEMX0"
      },
      "outputs": [],
      "source": [
        "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
        "epochs = 200\n",
        "batches_done = 0\n",
        "list_loss_D = []\n",
        "list_loss_G = []\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    for i, (imgs, _) in enumerate(train_loader):\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = Variable(imgs.type(Tensor))\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        D_optimizer.zero_grad()\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], z_dim))))\n",
        "\n",
        "        # Generate a batch of images\n",
        "        fake_imgs = G(z).detach()\n",
        "        # Adversarial loss\n",
        "        d_loss = -torch.mean(D(real_imgs)) + torch.mean(D(fake_imgs))\n",
        "\n",
        "        d_loss.backward()\n",
        "        D_optimizer.step()\n",
        "\n",
        "        # Train the generator every n_critic iterations\n",
        "        if i % n_critic == 0:\n",
        "\n",
        "            # -----------------\n",
        "            #  Train Generator\n",
        "            # -----------------\n",
        "\n",
        "            G_optimizer.zero_grad()\n",
        "\n",
        "            # Generate a batch of images\n",
        "            gen_imgs = G(z)\n",
        "            # Adversarial loss\n",
        "            g_loss = -torch.mean(D(gen_imgs))\n",
        "\n",
        "            g_loss.backward()\n",
        "            G_optimizer.step()\n",
        "\n",
        "            # add loss to list\n",
        "            list_loss_D.append(d_loss.item())\n",
        "            list_loss_G.append(g_loss.item())\n",
        "        \n",
        "        if i % 300 == 0:\n",
        "            print(\n",
        "              \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "              % (epoch, epochs, i, len(train_loader), d_loss.item(), g_loss.item()))\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        save_image(gen_imgs.view(gen_imgs.size(0), 1, 28, 28), os.path.join(sample_dir, \"%d.png\" % epoch), nrow=5, normalize=True)\n",
        "\n",
        "torch.save(G, os.path.join(ROOT, 'G.pt'))\n",
        "torch.save(D, os.path.join(ROOT, 'D.pt'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "WGAN-SN.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}